{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Project_2_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RJUDMX3R5wgu","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ReNSjb9ehlP","colab_type":"code","colab":{}},"source":["!pip install tensorflow==1.14.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iw1gXcS8Ki2P","colab_type":"code","colab":{}},"source":["#import libraries\n","import time\n","import pandas as pd\n","import csv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import nltk\n","import re\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","#Data Cleaning\n","from nltk.corpus import stopwords\n","from keras.layers.embeddings import Embedding\n","from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n","from keras.layers.core import Dense, Dropout, Activation, Lambda\n","import string\n","#Model Evaludation\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics\n","#Model Feature Extraction\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","from keras.models import Sequential"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-Fx9KqCLBF-","colab_type":"code","colab":{}},"source":["#Stemmer and Lemmatizer intialization\n","porter = PorterStemmer()\n","lancaster=LancasterStemmer()\n","wordnet_lemmatizer = WordNetLemmatizer()\n","stopwords_en = stopwords.words(\"english\")\n","punctuations=\"?:!.,;'\\\"-()*\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Piaw3a3KVEuE","colab_type":"code","colab":{}},"source":["import re\n","def newdataclean(uncleandata):\n","  cleanphrase=[]\n","  for x in range(0,len(uncleandata.values)):\n","    tempphrase=uncleandata.values[x].lower()\n","    tempphrase=re.sub('[^a-zA-Z]',' ',tempphrase)\n","    tempphrase=tempphrase.strip()\n","    cleanphrase.append(tempphrase)\n","    \n","  return cleanphrase"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"phHU6anyLGrj","colab_type":"code","colab":{}},"source":["#utility function to clean the dataset\n","def datacleaning(remove_stopwords,useStemming,useLemma,removePuncs,newdata):\n","  cleanReview=[]\n","  for x in range(0,len(newdata.values)):\n","    tmpReview=[]\n","    for w in nltk.word_tokenize(newdata.values[x]):\n","        newWord = str(w).lower() #Set newWork to be the updated word\n","        if remove_stopwords and (w in stopwords_en):#if the word is a stopword & we want to remove stopwords\n","            continue #skip the word and don’t had it to the normalized review\n","        if removePuncs and (w in punctuations):#if the word is a punc. & we want to remove punctuations\n","            continue #skip the word and don’t had it to the normalized review\n","        if useStemming: #if useStemming is set to True\n","            #Keep one stemmer commented out\n","            #newWord = porter.stem(newWord) #User porter stemmer\n","            newWord = lancaster.stem(newWord) #Use Lancaster stemmer\n","        if useLemma:\n","            newWord = wordnet_lemmatizer.lemmatize(newWord)\n","        tmpReview.append(newWord) #Add normalized word to the tmp review\n","    cleanReview.append(' '.join(tmpReview))\n","  return cleanReview"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NU7SYSAqLQ4O","colab_type":"code","colab":{}},"source":["url=\"/content/drive/My Drive/NLP/Combined_News_DJIA.csv\"\n","dataset = pd.read_csv(url,encoding = \"ISO-8859-1\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjlwsVjsLglq","colab_type":"code","colab":{}},"source":["dataset.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gZ9AefuLimW","colab_type":"code","colab":{}},"source":["dataset.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGS7IhLTMI4B","colab_type":"code","colab":{}},"source":["#Count of Each Sentiment\n","df_plot=dataset['Label'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIJ4HHWnMNqA","colab_type":"code","colab":{}},"source":["#Plot the sentiment class count\n","plt.style.use(\"ggplot\")\n","plt.figure(figsize=(5,4))\n","fig = dataset.groupby('Label').Top1.count().plot.bar(ylim=0, width=0.4)\n","plt.title('Label Count')\n","plt.ylabel('Number of Occurrences', fontsize=12)\n","plt.xlabel('Sentiment', fontsize=12)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rebtNEY7MZAm","colab_type":"code","colab":{}},"source":["#Split the dataset in Test Train\n","X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,2:],dataset['Label'], test_size=0.3, random_state=2003)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAn2lYh_w9et","colab_type":"code","colab":{}},"source":["Y_train.value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KraEXU-HeUKP","colab_type":"code","colab":{}},"source":["X_train.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGCZpGg66Dwl","colab_type":"text"},"source":["Train Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"JB0QHTYBohnK","colab_type":"code","colab":{}},"source":["#Combine all the top heading in a list\n","headlines = []\n","for row in range(0,len(X_train.index)):\n","    headlines.append(' '.join(str(x) for x in X_train.iloc[row,0:]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OwGxj0sjpzm2","colab_type":"code","colab":{}},"source":["#Add the list as column to the X_train dataframe\n","X_train['Combined']=headlines"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"psyHMEj2wfTl","colab_type":"code","colab":{}},"source":["X_train.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPs7whmwcl5l","colab_type":"code","colab":{}},"source":["#Final Train data frame\n","train_data=pd.concat([X_train['Combined'],Y_train],axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLtHQEepqF5p","colab_type":"code","colab":{}},"source":["train_data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ycfu0ysUp7Ie","colab_type":"code","colab":{}},"source":["train_data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9ONj7OsdFue","colab_type":"code","colab":{}},"source":["#clean the data and concat the clean text to the train_data\n","temp_train_phrase=datacleaning(True,False,True,True,train_data['Combined'])\n","train_data['Cleaned']=temp_train_phrase"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-wfRX8gdZ_t","colab_type":"code","colab":{}},"source":["train_data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dcb_Lq3JghVR","colab_type":"code","colab":{}},"source":["train_data['Label'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBqwuIwoHu25","colab_type":"code","colab":{}},"source":["#Training paramteters intialization\n","max_features = 20000\n","maxlen = 200"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kbkeorMt7VDc","colab_type":"text"},"source":["Feature Generation"]},{"cell_type":"code","metadata":{"id":"mVvzn0vaHxBm","colab_type":"code","colab":{}},"source":["# vectorize the text samples into a 2D integer tensor\n","from keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer(nb_words=max_features)\n","tokenizer.fit_on_texts(train_data['Cleaned'])\n","sequences_train = tokenizer.texts_to_sequences(train_data['Cleaned'])\n","#sequences_test = tokenizer.texts_to_sequences(testheadlines)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzL9mqJoIKnk","colab_type":"code","colab":{}},"source":["#Padding the trainin data\n","from keras.preprocessing import sequence\n","from keras.utils import np_utils\n","from keras.optimizers import SGD,Adam,Nadam\n","from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n","from keras.layers import SpatialDropout1D\n","X_train_pad = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n","\n","Y_train_pad = np_utils.to_categorical(train_data['Label'], 2)\n","print('X_train shape:', X_train_pad.shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwXo168k690J","colab_type":"text"},"source":["Model Training"]},{"cell_type":"code","metadata":{"id":"x8fGYuTlFulR","colab_type":"code","colab":{}},"source":["from keras import backend as K\n","def recall_m(y_true, y_pred):\n","  true_positives = K.sum(K.round(K.clip(y_true*y_pred, 0, 1)))\n","  possible_positives = K.sum(K.round(K.clip(y_true, 0 ,1)))\n","  recall = true_positives / (possible_positives + K.epsilon())\n","  return recall\n","\n","def precision_m(y_true, y_pred):\n","  true_positives = K.sum(K.round(K.clip(y_true*y_pred, 0, 1)))\n","  predicted_positives = K.sum(K.round(K.clip(y_pred, 0 ,1)))\n","  precision = true_positives / (predicted_positives + K.epsilon())\n","  return precision\n","\n","def f1_m(y_true, y_pred):\n","  precision = precision_m(y_true, y_pred)\n","  recall = recall_m(y_true, y_pred)\n","  return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QzaPGClIqTW","colab_type":"code","colab":{}},"source":["#opt = SGD(lr=0.001, momentum=0.5)\n","#opt = Adam(lr=0.0001)\n","model = Sequential()\n","model.add(Embedding(max_features, 64))\n","model.add(SpatialDropout1D(0.5))\n","model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5)) \n","model.add(Dense(2))\n","model.add(Activation('sigmoid'))\n","#compile the model\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',f1_m,precision_m,recall_m])\n","#Train the model\n","modelhistory=model.fit(X_train_pad, Y_train_pad, batch_size=32, epochs=3,validation_split=0.2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8xlQfUzj2FJ","colab_type":"code","colab":{}},"source":["# Model Diagram\n","\n","from keras.utils.vis_utils import plot_model  \n","plot_model(model, to_file='/content/drive/My Drive/NLP/model_plot.png', show_shapes=True, show_layer_names=True) \n","from IPython.display import Image\n","Image(retina=True, filename='/content/drive/My Drive/NLP/model_plot.png')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DhYMJXI7KIQ","colab_type":"text"},"source":["Model Test"]},{"cell_type":"code","metadata":{"id":"2pz2Poznr7sJ","colab_type":"code","colab":{}},"source":["X_test.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRXVLSW0xMwA","colab_type":"code","colab":{}},"source":["X_test.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLWqNz547OZJ","colab_type":"text"},"source":["Test Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"7F2KT4GQr-b4","colab_type":"code","colab":{}},"source":["#Combine the top headline in to list for test data\n","headlines_test = []\n","for row in range(0,len(X_test.index)):\n","    headlines_test.append(' '.join(str(x) for x in X_test.iloc[row,0:]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z63REnTfsuT_","colab_type":"code","colab":{}},"source":["#Adding the combined healines as column to test data\n","X_test['Combined']=headlines_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmNuxIsos5h7","colab_type":"code","colab":{}},"source":["#New dataframe having only required data\n","test_data=pd.concat([X_test['Combined'],Y_test],axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6JvBzwBhmu4","colab_type":"code","colab":{}},"source":["test_data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7t0zSJ_IuTwC","colab_type":"code","colab":{}},"source":["X_test.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSrMhZYSuWmZ","colab_type":"code","colab":{}},"source":["test_data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AdyA20KxfiT","colab_type":"code","colab":{}},"source":["test_data['Label'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtpSzqc2g7Yd","colab_type":"code","colab":{}},"source":["#Clean the test data and add the clean data as column to test_data\n","temp_test_phrase=datacleaning(True,False,True,True,test_data['Combined'])\n","test_data['Cleaned']=temp_test_phrase"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sgt2zgLUhxSg","colab_type":"code","colab":{}},"source":["test_data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfGA0yuFIw_w","colab_type":"code","colab":{}},"source":["#Padding the test data\n","sequences_test = tokenizer.texts_to_sequences(test_data['Cleaned'])\n","X_test_pad = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n","Y_test_pad = np_utils.to_categorical(test_data['Label'], 2)\n","print('X_test_pad shape:', X_test_pad.shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KjgdKtWuAmRT","colab_type":"code","colab":{}},"source":["X_test_pad"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WtD9Q78v7xab","colab_type":"text"},"source":["Model test and performance"]},{"cell_type":"code","metadata":{"id":"3FO-Ey6HJDdV","colab_type":"code","colab":{}},"source":["#score, acc = model.evaluate(X_test_pad, Y_test_pad,batch_size=batch_size)\n","#print('Test score:', score)\n","#print('Test accuracy:', acc)\n","print(model.metrics_names)\n","model.evaluate(X_test_pad,Y_test_pad,batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l15MQhvs8sWy","colab_type":"text"},"source":["For Saving and Loading the model"]},{"cell_type":"code","metadata":{"id":"3reCLCUu4DX6","colab_type":"code","colab":{}},"source":["# For saving the model, uncomment the below lines\n","#!apt-get install libhdf5-serial-dev\n","#import h5py\n","#model.save('/content/drive/My Drive/NLP/NLP_Project/Model/10_project_2_TT.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCKh65bV8efh","colab_type":"code","colab":{}},"source":["# For loading the model, uncomment the below lines\n","#from keras.models import load_model\n","#!apt-get install libhdf5-serial-dev\n","#import h5py\n","#model = load_model('/content/drive/My Drive/NLP/NLP_Project/Model/10_project_2_TT.h5', custom_objects={'f1_m': f1_m,'precision_m':precision_m,'recall_m':recall_m})"],"execution_count":0,"outputs":[]}]}